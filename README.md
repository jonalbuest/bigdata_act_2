# **Big Data - Actividad 2: Heart Disease 2020 Dataset**

Toda la documentaci√≥n y c√≥digo fuente asociado a la Actividad 2 del curso de Big Data, centrada en el dise√±o e implementaci√≥n de un modelo de base de datos normalizado para el an√°lisis del dataset Heart Disease 2020.

Revisar Notebook: [src/Actividad_2_Procesamiento_Infraestructura_Cloud.ipynb](src/Actividad_2_Procesamiento_Infraestructura_Cloud.ipynb)

# 1- Descripci√≥n del Proyecto

Este proyecto implementa un modelo de base de datos normalizado para el an√°lisis del dataset Heart Disease 2020, enfocado en factores de riesgo cardiovascular y caracter√≠sticas de salud de la poblaci√≥n.

# 2- Estructura del Proyecto

```
bigdata_act_2/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ doc/
‚îÇ   ‚îú‚îÄ‚îÄ diccionario_datos.md    # Documentaci√≥n completa del esquema
‚îÇ   ‚îî‚îÄ‚îÄ modelo.drawio            # Diagrama del modelo de datos
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ schema_spark_sql.sql     # DDL en Spark SQL
    ‚îî‚îÄ‚îÄ schema_pyspark.py        # StructType en PySpark
```

# 3- Dise√±o del Esquema de la Base de Datos

### 3.1 - Modelo de Datos (Unity Catalog)

El modelo sigue la jerarqu√≠a de Unity Catalog con 4 tablas en **3ra Forma Normal (3NF)**:

### 3.2 - Jerarqu√≠a de Objetos

#### 3.2.1. **Metastore** (Nivel Superior)

- **Nombre**: `heart_disease_metastore`
- **Prop√≥sito**: Contenedor de nivel superior para gobernanza centralizada

#### 3.2.2. **Catalog** (Segundo Nivel)

- **Nombre**: `heart_disease_catalog`
- **Prop√≥sito**: Agrupaci√≥n l√≥gica de schemas de salud cardiovascular

#### 3.2.3. **Schema** (Tercer Nivel)

- **Nombre**: `heart2020_schema`
- **Prop√≥sito**: Base de datos l√≥gica con tablas del dataset 2020

#### 3.2.4. **Tables** (Cuarto Nivel)

1. **persona** - Datos demogr√°ficos y de salud general
2. **habitos** - H√°bitos y estilo de vida
3. **condiciones_medicas** - Condiciones m√©dicas preexistentes
4. **salud_cardiaca** - Informaci√≥n de salud cardiovascular

### Diagrama de Unity Catalog

```mermaid
graph TB
    subgraph Metastore["Metastore: heart_disease_metastore"]
        subgraph Catalog["Catalog: heart_disease_catalog"]
            subgraph Schema["Schema: heart2020_schema"]
                T1["Table: persona"]
                T2["Table: habitos"]
                T3["Table: condiciones_medicas"]
                T4["Table: salud_cardiaca"]
            end
        end
    end

    T1 -.->|FK| T2
    T1 -.->|FK| T3
    T1 -.->|FK| T4

    style Metastore fill:#e1f5ff,stroke:#0066cc,stroke-width:3px
    style Catalog fill:#fff4e6,stroke:#ff9800,stroke-width:2px
    style Schema fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
    style T1 fill:#e8f5e9,stroke:#4caf50,stroke-width:1px
    style T2 fill:#e8f5e9,stroke:#4caf50,stroke-width:1px
    style T3 fill:#e8f5e9,stroke:#4caf50,stroke-width:1px
    style T4 fill:#e8f5e9,stroke:#4caf50,stroke-width:1px
```

### Diagrama Entidad-Relaci√≥n

```mermaid
erDiagram
    persona ||--|| habitos : tiene
    persona ||--|| condiciones_medicas : tiene
    persona ||--|| salud_cardiaca : tiene

    persona {
        INTEGER id_persona PK
        TEXT Sex
        TEXT AgeCategory
        TEXT Race
        FLOAT BMI
        FLOAT SleepTime
        TEXT GenHealth
        FLOAT PhysicalHealth
        FLOAT MentalHealth
        TEXT DiffWalking
    }

    habitos {
        INTEGER id_habito PK
        TEXT Smoking
        TEXT AlcoholDrinking
        TEXT PhysicalActivity
        TEXT Diabetic
        INTEGER id_persona FK
    }

    condiciones_medicas {
        INTEGER id_condicion PK
        TEXT Stroke
        TEXT Asthma
        TEXT KidneyDisease
        TEXT SkinCancer
        INTEGER id_persona FK
    }

    salud_cardiaca {
        INTEGER id_saludcardiaca PK
        TEXT HeartDisease
        INTEGER id_persona FK
    }
```

### 3.3 - Diccionario de Datos

Consulta [`doc/diccionario_datos.md`](doc/diccionario_datos.md) para:

- Arquitectura completa de Unity Catalog
- Descripci√≥n detallada de cada tabla y campo
- Tipos de datos y restricciones
- Relaciones entre tablas
- Diagramas Mermaid completos
- Configuraci√≥n de gobernanza y permisos

## 4- Configuraci√≥n de Cluster en Databricks

Para implementar este esquema en Databricks se elabor√≥ un cluster siguiendo los pasos detallados en el documento [doc/creacion_cluster_databricks.md](doc/creacion_cluster_databricks.md).

## 5- Ingesta de datos desde Kaggle y Creaci√≥n de tablas en Databricks usando SQL

Empleamos el siguiente DDL en Spark SQL para crear el cat√°logo, schema y tablas en Databricks Unity Catalog:

todo el siguiente c√≥digo va en un notebook de Databricks con lenguaje SQL

```SQL

%sql
-- ============================================================================
-- PASO 1: Crear el Cat√°logo (Catalog)
-- ============================================================================
CREATE CATALOG IF NOT EXISTS heart_disease_catalog
COMMENT 'Cat√°logo para an√°lisis de salud cardiovascular y factores de riesgo';

-- Usar el cat√°logo creado
USE CATALOG heart_disease_catalog;

-- ============================================================================
-- PASO 2: Crear el Schema
-- ============================================================================
CREATE SCHEMA IF NOT EXISTS heart2020_schema
COMMENT 'Schema para dataset Heart Disease 2020 - CDC BRFSS'
WITH DBPROPERTIES (
    'project' = 'heart_disease_analysis',
    'dataset_year' = '2020',
    'source' = 'CDC_BRFSS',
    'created_by' = 'Jonathan Albu√©s',
    'version' = '1.0'
);
```

Ver esquema completo en [`src/schema_spark_sql.sql`](src/schema_spark_sql.sql).

### 5.1 - Mostrar la descripci√≥n del esquema y las tablas creadas

![Descripci√≥n del esquema 1](doc/photo/descripcion_1.png)

---

![Descripci√≥n del esquema 2](doc/photo/descripcion_2.png)

---

![Descripci√≥n del esquema 3](doc/photo/descripcion_3.png)

## 5.2 - Obteniendo datos de kaggle

Se obtuvieron los datos del dataset Heart Disease 2020 desde Kaggle en el siguiente enlace:

`kamilpytlak/personal-key-indicators-of-heart-disease`

El c√≥digo API de Kaggle se utiliz√≥ para descargar el dataset en formato ZIP y luego se extrajeron los archivos CSV para su posterior procesamiento.

En este caso el dataset contaba con dos carpetas internas, cada una con un archivo CSV diferente. En nuestro caso utilizamos el archivo `heart_2020_cleaned.csv` que se encontraba en la carpeta `2020`.

![Datos descargados de Kaggle](doc/photo/Datos_kaggle.png)

Usando pandas se carg√≥ el archivo CSV y se realiz√≥ una inspecci√≥n inicial de los datos para entender su estructura y contenido, usando `display()`:

```python
# 3. Leer DF de las subcarpetas
df2020 = pd.read_csv(csv_files[0])
df2022 = pd.read_csv(csv_files[1])
display(df2020.head())
display(df2022.head())
```

![Vista previa de datos con pandas](doc/photo/Datos_kaggle_2.png)

Luego usando Spark se cargaron los datos en un DataFrame de Spark para su posterior procesamiento y an√°lisis:

```python
spark_df = spark.createDataFrame(df2020)
```

![DataFrame de Spark creado](doc/photo/Data_kaggle_3.png)

Finalmente se cre√≥ una tabla temporal llamada `heart_2020_cleaned`:

```python
spark_df.createTempView("heart_2020_cleaned")
display(spark_df)
```

![Tabla temporal creada](doc/photo/Data_kaggle_4.png)

Contar los registros del DataFrame:

![alt text](doc/photo/Data_kaggle_5.png)

### 5.3 - Creaci√≥n de IDs √∫nicos para cada registro

```python
from pyspark.sql.functions import row_number, monotonically_increasing_id, col
from pyspark.sql.window import Window

# Generar IDs √∫nicos para cada registro
window_spec = Window.orderBy(monotonically_increasing_id())
df_with_id = spark_df.withColumn("id_persona", row_number().over(window_spec))

print(f"IDs generados para {df_with_id.count():,} registros")
df_with_id.select("id_persona", "Sex", "AgeCategory", "BMI", "HeartDisease").show(10)

```

### 5.4 - Filtrado de DataFrame para las tablas PERSONA, HABITOS, CONDICIONES_MEDICAS y SALUD_CARDIACA

Tabla PERSONA:

![Generaci√≥n de IDs √∫nicos](doc/photo/Data_kaggle_6.png)

Tabla HABITOS:

![Filtrado para tabla HABITOS](doc/photo/Data_kaggle_7.png)

Tabla CONDICIONES_MEDICAS:

![Filtrado para tabla CONDICIONES_MEDICAS](doc/photo/Data_kaggle_8.png)

Tabla SALUD_CARDIACA:

![Filtrado para tabla SALUD_CARDIACA](doc/photo/Data_kaggle_9.png)

---

## 6- Validaci√≥n y An√°lisis Exploratorio de Datos (EDA)

### 6.1 - Metadatos

**Prop√≥sito:** Verificar estructura de tablas, tipos de datos y restricciones.

- **DESCRIBE TABLE**: Valida columnas, tipos (INT, STRING, DOUBLE) y PRIMARY KEYs
- **SHOW CREATE TABLE**: Confirma DDL completo y formato Delta Lake
- **printSchema()**: Verifica esquema en Spark DataFrame

### 6.2 - Descripci√≥n de Datos

**Prop√≥sito:** Detectar anomal√≠as y entender distribuciones num√©ricas.

- **describe()**: Estad√≠sticas (count, mean, stddev, min, max) - BMI promedio: 28.32 (sobrepeso)

![alt text](doc/photo/descripcion_std_pySpark.png)


- **Funciones SQL**: AVG, STDDEV, PERCENTILE - Mediana BMI: 27.34 (distribuci√≥n sesgada derecha)

![alt text](doc/photo/descripcion_std_pySpark_2.png)

### 6.3 - Consultas SELECT y GROUP BY

**Prop√≥sito:** Validar integridad referencial y distribuciones categ√≥ricas.

- **SQL GROUP BY**: Enfermedades card√≠acas por sexo (Hombres: 10.62% vs Mujeres: 6.69%)

![alt text](doc/photo/Consultas_SELECT_BYGROUP.png)

- **PySpark equivalente**: Mismos resultados - confirma consistencia entre interfaces
![alt text](doc/photo/Consulta_Pyspark_Groupby.png)


### 6.4 - Conteos y Muestras

**Prop√≥sito:** Confirmar carga completa sin p√©rdida de datos.

- **COUNT(\*)**: Las 4 tablas tienen 319,795 registros (relaci√≥n 1:1 verificada)
- **LIMIT + WHERE**: Muestreo de casos espec√≠ficos (ej: HeartDisease = 'Yes')
- **Filtros PySpark**: ~15,000 personas con obesidad + enfermedad card√≠aca

---

## 7- Comparaci√≥n: SQL vs Spark (PySpark)

### 7.1 - Sistema de Puntuaci√≥n por Criterio

Escala: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5 estrellas = excelente, 1 estrella = limitado)

| Criterio                      | SQL        | Spark (PySpark) | Ganador      |
| ----------------------------- | ---------- | --------------- | ------------ |
| **Facilidad de uso**          | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê          | üèÜ **SQL**   |
| **Curva de aprendizaje**      | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê            | üèÜ **SQL**   |
| **Escalabilidad**             | ‚≠ê‚≠ê       | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | üèÜ **Spark** |
| **Procesamiento distribuido** | ‚≠ê         | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | üèÜ **Spark** |
| **Integraci√≥n con BI**        | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê          | üèÜ **SQL**   |
| **UDFs personalizadas**       | ‚≠ê‚≠ê       | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | üèÜ **Spark** |
| **Pipelines complejos**       | ‚≠ê‚≠ê       | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | üèÜ **Spark** |
| **Machine Learning**          | ‚≠ê         | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | üèÜ **Spark** |
| **Optimizaci√≥n autom√°tica**   | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê          | üèÜ **SQL**   |
| **Portabilidad de c√≥digo**    | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê          | üèÜ **SQL**   |
| **Debugging**                 | ‚≠ê‚≠ê‚≠ê     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | üèÜ **Spark** |
| **Integraci√≥n con Python**    | ‚≠ê‚≠ê       | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | üèÜ **Spark** |

#### üìä Puntuaci√≥n Total:

- **SQL:** 37/60 estrellas ‚≠ê (61.7%)
- **Spark (PySpark):** 46/60 estrellas ‚≠ê (76.7%)

---

### 7.2 - An√°lisis por Caso de Uso

#### üéØ Cu√°ndo usar **SQL** (5 casos ideales):

1. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **An√°lisis exploratorio r√°pido** - Sintaxis simple, resultados inmediatos
2. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Reportes para stakeholders** - Universalmente entendido
3. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Integraci√≥n con dashboards BI** - Compatible con Tableau, Power BI
4. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Consultas ad-hoc simples** - Menos verboso que PySpark
5. ‚≠ê‚≠ê‚≠ê‚≠ê **Validaci√≥n de datos** - DESCRIBE, COUNT, GROUP BY intuitivos

#### üöÄ Cu√°ndo usar **Spark (PySpark)** (5 casos ideales):

1. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Procesamiento masivo (>100GB)** - Escalabilidad distribuida nativa
2. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Pipeline ETL automatizado** - Reutilizaci√≥n de c√≥digo, control de flujo
3. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Machine Learning** - MLlib integrado (clasificaci√≥n, regresi√≥n, clustering)
4. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Transformaciones complejas** - UDFs con cualquier librer√≠a Python
5. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Integraci√≥n con ecosistema Python** - pandas, numpy, scikit-learn

---

### 7.3 - Ventajas y Limitaciones

#### ‚úÖ **SQL - Ventajas**

- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Sintaxis declarativa universalmente conocida
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Catalyst Optimizer autom√°tico (sin ajustes manuales)
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Portabilidad entre plataformas (Databricks, Snowflake, BigQuery)
- ‚≠ê‚≠ê‚≠ê‚≠ê Window functions intuitivas (PERCENTILE, RANK, ROW_NUMBER)

#### ‚ùå **SQL - Limitaciones**

- ‚≠ê‚≠ê UDFs limitadas seg√∫n motor SQL (no est√°ndar)
- ‚≠ê‚≠ê Dif√≠cil mantener pipelines multi-etapa
- ‚≠ê Sin capacidades ML nativas
- ‚≠ê‚≠ê Escalabilidad limitada sin cluster distribuido

#### ‚úÖ **Spark (PySpark) - Ventajas**

- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Procesamiento distribuido con miles de nodos
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê APIs ricas (DataFrame/RDD) con control fino
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê UDFs flexibles con cualquier librer√≠a Python
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê MLlib integrado para Machine Learning
- ‚≠ê‚≠ê‚≠ê‚≠ê Type safety y validaci√≥n en IDE

#### ‚ùå **Spark (PySpark) - Limitaciones**

- ‚≠ê‚≠ê Curva de aprendizaje pronunciada
- ‚≠ê‚≠ê‚≠ê Requiere ajustes manuales de rendimiento (cache, repartition)
- ‚≠ê‚≠ê‚≠ê Menos portable que SQL puro
- ‚≠ê‚≠ê M√°s verboso para consultas simples

---

### 7.4 - Mejor Pr√°ctica: Combinar SQL y PySpark ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

```python
# 1. Usar PySpark para transformaciones complejas
df_cleaned = spark_df.filter(col("BMI").isNotNull()) \
                     .withColumn("obesity", when(col("BMI") > 30, 1).otherwise(0))

# 2. Guardar como tabla temporal
df_cleaned.createOrReplaceTempView("temp_cleaned")

# 3. Usar SQL para an√°lisis final (m√°s legible)
result = spark.sql("""
    SELECT
        AgeCategory,
        AVG(obesity) as tasa_obesidad,
        COUNT(*) as total
    FROM temp_cleaned
    GROUP BY AgeCategory
    ORDER BY tasa_obesidad DESC
""")
display(result)
```

**Resultado:** Lo mejor de ambos mundos üöÄ (Puntuaci√≥n combinada: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
